---
title: "p8105_hw5_xl3494"
author: Selina Lyu
output: github_document
---
```{r, include = FALSE}
library(tidyverse)
library(broom)
set.seed(1)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = 0.6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_color_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

## Problem 1

Write a function that, for a fixed group size, randomly draws “birthdays” for each person; checks whether there are duplicate birthdays in the group; and returns TRUE or FALSE based on the result.

```{r}
share_birthday = function(n) {
  
  birthdays = sample(1:365, size = n, replace = TRUE) 
  return(length(unique(birthdays)) < n)           
  
  }
```

Next, run this function 10000 times for each group size between 2 and 50. For each group size, compute the probability that at least two people in the group will share a birthday by averaging across the 10000 simulation runs.

```{r}
prob_results = 
  expand_grid(
    group_size = 2:50,
    iter = 1:10000) |> 
  mutate(
    share = map_lgl(group_size, share_birthday)) |>
  group_by(group_size) |>
  summarize(prob_share = mean(share))

prob_results
```

Make a plot showing the probability as a function of group size, and comment on your results.

```{r}
ggplot(prob_results, aes(x = group_size, y = prob_share)) +
  geom_line() +
  geom_point() +
  labs(
    title = "Probability of Shared Birthday vs Group Size",
    x = "Group Size (Number of People)",
    y = "Probability"
  )
```
This plot shows the probabilities that at least two people share a birthday for group size of 2 to 50. When group size is small (<15), the probabilities are low. The increase in probabilities becomes faster when group size increases to 15-35. The probabilities approach to 1 as the group size further increases to 50.

## Problem 2

Function of μ and run 5000 datasets for μ = 0:6
```{r}
sim_test = function(mu) {
  
  x = rnorm(n = 30, mean = mu, sd = 5)
  ttest = t.test(x, mu = 0)
  tidy(ttest) |>
    select(estimate, p.value) |>
    rename(mu_hat = estimate) |>
    as_tibble()
  
}

test_result = 
  expand_grid(
    true_mu = 0:6, 
    iter = 1:5000
  ) |>
  mutate(output = map(true_mu, sim_test)) |>
  unnest(output)

test_result
```

Make a plot showing the proportion of times the null was rejected (the power of the test) on the y axis and the true value of μ on the x axis

```{r}
power_plot = 
  test_result |>
  mutate(reject = p.value < 0.05) |>
  group_by(true_mu) |>
  summarize(power = mean(reject)) |>
  ggplot(aes(x = true_mu, y = power)) +
  geom_line()+
  geom_point() +
  labs(
    title = "Power vs True Mean (μ)",
    x = expression(True~mu),
    y = "Power (Probability of Rejecting Null)"
  )

power_plot
```

The plot shows the relationship between the true mean (μ) and the power of the one-sample t-test. When μ = 0, the power is about 0.05, matching the significance level (α = 0.05) because the null hypothesis is true and should rarely be rejected. As μ increases, the power rises sharply. As true mean increases, the power approaches 1, indicating the test almost always rejects the null when the true mean is far from zero.

A plot showing the average estimate of μ vs. true μ
and average estimate of μ only in samples for which the null was rejected vs. true μ
```{r}
avg_mu = 
  test_result |>
  group_by(true_mu) |>
  summarize(
    mean_all = mean(mu_hat),
    mean_rejected = mean(mu_hat[p.value < 0.05])
  ) |>
  ggplot(aes(x = true_mu)) +
  geom_line(aes(y = mean_all, color = "All samples")) +
  geom_point(aes(y = mean_all, color = "All samples")) +
  geom_line(aes(y = mean_rejected, color = "Rejected only")) +
  geom_point(aes(y = mean_rejected, color = "Rejected only")) +
  labs(
    title = expression("Average Estimate of"~hat(mu)~"vs True"~mu),
    x = expression(True~mu),
    y = expression("Average"~hat(mu))
  )

avg_mu
```

The "All Samples" line shows that the one-sample t-test provides an unbiased estimator — on average, the sample mean equals the true μ. However, the "Rejected only" line sit above the purple line, especially when true μ is small, showing that when we only look at simulations where the null hypothesis was rejected, the average μ̂ is larger than the true μ .This happens because significant results tend to occur when random sampling error pushes the estimated mean farther from zero. As true μ, the bias decreases because nearly all tests reject the null, and selection plays a smaller role.


## Problem 3

Import and describe the raw data
```{r}
hom_raw = readr::read_csv("https://raw.githubusercontent.com/washingtonpost/data-homicides/master/homicide-data.csv")
```

There are 52179 homicide records in this dataset. Each row is an unique homicide case with uid, reported date, victim info (first name, race, age, and sex), homicide location (city and state, latitude/longitude), and case disposition.

Summarize within cities to obtain the total number of homicides and the number of unsolved homicides
```{r}
city_sum =
  hom_raw |>
  mutate(city_state = str_c(city,",", state),
         unsolved = disposition %in% c("Closed without arrest", "Open/No arrest")) |>
  group_by(city_state) |>
  summarize(
    total = n(),
    unsolved_n = sum(unsolved)
  )

city_sum
```

For the city of Baltimore, MD, estimate the proportion of homicides that are unsolved

```{r}
balt = 
  city_sum |> 
  filter(city_state == "Baltimore,MD")

balt_test = 
  prop.test(x = balt$unsolved_n, n = balt$total)

balt_tidy = 
  tidy(balt_test) |>
  select(estimate, conf.low, conf.high)

balt_tidy
```

The estimated portion of unsolved homicides in Baltimore, MD is 0.646 (95% CI: 0.628 - 0.663).

Now run prop.test for each of the cities in your dataset, and extract both the proportion of unsolved homicides and the confidence interval for each.

```{r}
city_ci =
  city_sum |>
  mutate(tst = map2(unsolved_n, total, ~ prop.test(.x, .y)),
         tidy = map(tst, tidy)) |>
  unnest(tidy) |>
  select(city_state, estimate, conf.low, conf.high) |>
  arrange(estimate) |>
  mutate(city_state = factor(city_state, levels = city_state))

city_ci
```

Create a plot that shows the estimates and CIs for each city

```{r}
city_plot = 
  city_ci |>
  ggplot(aes(x = city_state, y = estimate)) +
  geom_point() +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = 0.4) +
  labs(
    x = "City",
    y = "Estimated Portion Unresolved",
    title = "Unsolved homicide proportion by city"
  ) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))

city_plot
```


 









