p8105_hw5_xl3494
================
Selina Lyu

## Problem 1

Write a function that, for a fixed group size, randomly draws
“birthdays” for each person; checks whether there are duplicate
birthdays in the group; and returns TRUE or FALSE based on the result.

``` r
share_birthday = function(n) {
  
  birthdays = sample(1:365, size = n, replace = TRUE) 
  return(length(unique(birthdays)) < n)           
  
  }
```

Next, run this function 10000 times for each group size between 2 and
50. For each group size, compute the probability that at least two
people in the group will share a birthday by averaging across the 10000
simulation runs.

``` r
prob_results = 
  expand_grid(
    group_size = 2:50,
    iter = 1:10000) |> 
  mutate(
    share = map_lgl(group_size, share_birthday)) |>
  group_by(group_size) |>
  summarize(prob_share = mean(share))

prob_results
```

    ## # A tibble: 49 × 2
    ##    group_size prob_share
    ##         <int>      <dbl>
    ##  1          2     0.0024
    ##  2          3     0.0085
    ##  3          4     0.0167
    ##  4          5     0.0267
    ##  5          6     0.0399
    ##  6          7     0.0521
    ##  7          8     0.0751
    ##  8          9     0.0925
    ##  9         10     0.116 
    ## 10         11     0.139 
    ## # ℹ 39 more rows

Make a plot showing the probability as a function of group size, and
comment on your results.

``` r
ggplot(prob_results, aes(x = group_size, y = prob_share)) +
  geom_line() +
  geom_point() +
  labs(
    title = "Probability of Shared Birthday vs Group Size",
    x = "Group Size (Number of People)",
    y = "Probability"
  )
```

<img src="p8105_hw5_xl3494_files/figure-gfm/unnamed-chunk-4-1.png" width="90%" />
This plot shows the probabilities that at least two people share a
birthday for group size of 2 to 50. When group size is small (\<15), the
probabilities are low. The increase in probabilities becomes faster when
group size increases to 15-35. The probabilities approach to 1 as the
group size further increases to 50.

## Problem 2

Function of μ and run 5000 datasets for μ = 0:6

``` r
sim_test = function(mu) {
  
  x = rnorm(n = 30, mean = mu, sd = 5)
  ttest = t.test(x, mu = 0)
  tidy(ttest) |>
    select(estimate, p.value) |>
    rename(mu_hat = estimate) |>
    as_tibble()
  
}

test_result = 
  expand_grid(
    true_mu = 0:6, 
    iter = 1:5000
  ) |>
  mutate(output = map(true_mu, sim_test)) |>
  unnest(output)

test_result
```

    ## # A tibble: 35,000 × 4
    ##    true_mu  iter  mu_hat p.value
    ##      <int> <int>   <dbl>   <dbl>
    ##  1       0     1 -1.62    0.0545
    ##  2       0     2 -0.542   0.569 
    ##  3       0     3 -0.278   0.767 
    ##  4       0     4 -0.303   0.733 
    ##  5       0     5  0.406   0.637 
    ##  6       0     6  0.147   0.881 
    ##  7       0     7  1.20    0.191 
    ##  8       0     8 -0.0542  0.957 
    ##  9       0     9 -0.119   0.878 
    ## 10       0    10 -1.89    0.0712
    ## # ℹ 34,990 more rows

Make a plot showing the proportion of times the null was rejected (the
power of the test) on the y axis and the true value of μ on the x axis

``` r
power_plot = 
  test_result |>
  mutate(reject = p.value < 0.05) |>
  group_by(true_mu) |>
  summarize(power = mean(reject)) |>
  ggplot(aes(x = true_mu, y = power)) +
  geom_line()+
  geom_point() +
  labs(
    title = "Power vs True Mean (μ)",
    x = expression(True~mu),
    y = "Power (Probability of Rejecting Null)"
  )

power_plot
```

<img src="p8105_hw5_xl3494_files/figure-gfm/unnamed-chunk-6-1.png" width="90%" />

The plot shows the relationship between the true mean (μ) and the power
of the one-sample t-test. When μ = 0, the power is about 0.05, matching
the significance level (α = 0.05) because the null hypothesis is true
and should rarely be rejected. As μ increases, the power rises sharply.
As true mean increases, the power approaches 1, indicating the test
almost always rejects the null when the true mean is far from zero.

A plot showing the average estimate of μ vs. true μ and average estimate
of μ only in samples for which the null was rejected vs. true μ

``` r
avg_mu = 
  test_result |>
  group_by(true_mu) |>
  summarize(
    mean_all = mean(mu_hat),
    mean_rejected = mean(mu_hat[p.value < 0.05])
  ) |>
  ggplot(aes(x = true_mu)) +
  geom_line(aes(y = mean_all, color = "All samples")) +
  geom_point(aes(y = mean_all, color = "All samples")) +
  geom_line(aes(y = mean_rejected, color = "Rejected only")) +
  geom_point(aes(y = mean_rejected, color = "Rejected only")) +
  labs(
    title = expression("Average Estimate of"~hat(mu)~"vs True"~mu),
    x = expression(True~mu),
    y = expression("Average"~hat(mu))
  )

avg_mu
```

<img src="p8105_hw5_xl3494_files/figure-gfm/unnamed-chunk-7-1.png" width="90%" />

The “All Samples” line shows that the one-sample t-test provides an
unbiased estimator — on average, the sample mean equals the true μ.
However, the “Rejected only” line sit above the purple line, especially
when true μ is small, showing that when we only look at simulations
where the null hypothesis was rejected, the average μ̂ is larger than the
true μ .This happens because significant results tend to occur when
random sampling error pushes the estimated mean farther from zero. As
true μ, the bias decreases because nearly all tests reject the null, and
selection plays a smaller role.

## Problem 3

Import and describe the raw data

``` r
hom_raw = readr::read_csv("https://raw.githubusercontent.com/washingtonpost/data-homicides/master/homicide-data.csv")
```

    ## Rows: 52179 Columns: 12
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (9): uid, victim_last, victim_first, victim_race, victim_age, victim_sex...
    ## dbl (3): reported_date, lat, lon
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

There are 52179 homicide records in this dataset. Each row is an unique
homicide case with uid, reported date, victim info (first name, race,
age, and sex), homicide location (city and state, latitude/longitude),
and case disposition.

Summarize within cities to obtain the total number of homicides and the
number of unsolved homicides

``` r
city_sum =
  hom_raw |>
  mutate(city_state = str_c(city,",", state),
         unsolved = disposition %in% c("Closed without arrest", "Open/No arrest")) |>
  group_by(city_state) |>
  summarize(
    total = n(),
    unsolved_n = sum(unsolved)
  )

city_sum
```

    ## # A tibble: 51 × 3
    ##    city_state     total unsolved_n
    ##    <chr>          <int>      <int>
    ##  1 Albuquerque,NM   378        146
    ##  2 Atlanta,GA       973        373
    ##  3 Baltimore,MD    2827       1825
    ##  4 Baton Rouge,LA   424        196
    ##  5 Birmingham,AL    800        347
    ##  6 Boston,MA        614        310
    ##  7 Buffalo,NY       521        319
    ##  8 Charlotte,NC     687        206
    ##  9 Chicago,IL      5535       4073
    ## 10 Cincinnati,OH    694        309
    ## # ℹ 41 more rows

For the city of Baltimore, MD, estimate the proportion of homicides that
are unsolved

``` r
balt = 
  city_sum |> 
  filter(city_state == "Baltimore,MD")

balt_test = 
  prop.test(x = balt$unsolved_n, n = balt$total)

balt_tidy = 
  tidy(balt_test) |>
  select(estimate, conf.low, conf.high)

balt_tidy
```

    ## # A tibble: 1 × 3
    ##   estimate conf.low conf.high
    ##      <dbl>    <dbl>     <dbl>
    ## 1    0.646    0.628     0.663

The estimated portion of unsolved homicides in Baltimore, MD is 0.646
(95% CI: 0.628 - 0.663).

Now run prop.test for each of the cities in your dataset, and extract
both the proportion of unsolved homicides and the confidence interval
for each.

``` r
city_ci =
  city_sum |>
  mutate(tst = map2(unsolved_n, total, ~ prop.test(.x, .y)),
         tidy = map(tst, tidy)) |>
  unnest(tidy) |>
  select(city_state, estimate, conf.low, conf.high) |>
  arrange(estimate) |>
  mutate(city_state = factor(city_state, levels = city_state))
```

    ## Warning: There was 1 warning in `mutate()`.
    ## ℹ In argument: `tst = map2(unsolved_n, total, ~prop.test(.x, .y))`.
    ## Caused by warning in `prop.test()`:
    ## ! Chi-squared approximation may be incorrect

``` r
city_ci
```

    ## # A tibble: 51 × 4
    ##    city_state    estimate conf.low conf.high
    ##    <fct>            <dbl>    <dbl>     <dbl>
    ##  1 Tulsa,AL         0        0         0.945
    ##  2 Richmond,VA      0.263    0.223     0.308
    ##  3 Charlotte,NC     0.300    0.266     0.336
    ##  4 Memphis,TN       0.319    0.296     0.343
    ##  5 Tulsa,OK         0.331    0.293     0.371
    ##  6 Fresno,CA        0.347    0.305     0.391
    ##  7 Milwaukee,wI     0.361    0.333     0.391
    ##  8 Nashville,TN     0.362    0.329     0.398
    ##  9 Durham,NC        0.366    0.310     0.426
    ## 10 Sacramento,CA    0.370    0.321     0.421
    ## # ℹ 41 more rows

Create a plot that shows the estimates and CIs for each city

``` r
city_plot = 
  city_ci |>
  ggplot(aes(x = city_state, y = estimate)) +
  geom_point() +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = 0.4) +
  labs(
    x = "City",
    y = "Estimated Portion Unresolved",
    title = "Unsolved homicide proportion by city"
  ) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))

city_plot
```

<img src="p8105_hw5_xl3494_files/figure-gfm/unnamed-chunk-12-1.png" width="90%" />
